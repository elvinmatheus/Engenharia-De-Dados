{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6c6a04-152c-427c-a9d7-42ed9643dc5e",
   "metadata": {},
   "source": [
    "# Abstracting Data with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261021e-a865-403f-b82e-2bc20f883111",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Resilient Distributed Datasets (RDDs) are collections of immutable JVM objects that are distributed across an Apache Spark cluster.klyor error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972fa0e-fba0-4d19-9985-fac48912ee51",
   "metadata": {},
   "source": [
    "An RDD is the most fundamental dataset type of Apache Spark; any action on a Spark DataFrame eventually gets translated into a highly optimized execution of transformations and actions on RDDs.\n",
    "\n",
    "Data in an RDD is split into chunks based on a key and then dispersed across all the executor nodes. RDDs are highly resilient, that is, there are able to recover quickly from any issues as the same data chunks are replicated across multiple executor nodes. Thus, even if one executor fails, another will still process the data. This allows you to perform your functional calculations against your dataset very quickly by harnessing the power of multiple nodes. RDDs keep a log of all the execution steps applied to each chunk. This, on top of the data replication, speeds up the computations and, if anything goes wrong, RDDs can still recover the portion of the data lost due to an executor error.\n",
    "\n",
    "While it is common to lose a node in distributed environments (for example, due to connectivity issues, hardware problems), distribution and replication of the data defends against data loss, while data lineage allows the system to recover quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b580bba-8fbf-441f-b2a1-bf1b2eb8a52d",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "\n",
    "There are \n",
    "two ways to create an RDD in PySpark: you can either us \r\n",
    "the parallelize() methodâ€”a collection (list or an array of some elements)  r\r\n",
    "reference a file (or files) located either locally or through an exter al\r\n",
    "so.ipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e901cc80-0618-48d8-ab2d-14624c2a7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328c393-a038-4892-9a96-9aa886d612b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using parallelize() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d74d7-4826-42a5-ae46-0961a4b6cb5d",
   "metadata": {},
   "source": [
    "The following code snippet creates your RDD (myRDD) using\r\n",
    "the sc.parallelize() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ffe654e-d6c6-4ab2-b2da-c65a50b6efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize([('Mike', 19), ('June', 18), ('Rachel',16), ('Rob', 18),\n",
    "('Scott', 17)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6de008-4166-492f-b9cc-5ac3611ab3d4",
   "metadata": {},
   "source": [
    "To view what is inside your RDD, you can run the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44fbc36-4ca7-4434-82a4-39ec2ead6ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mike', 19), ('June', 18), ('Rachel', 16), ('Rob', 18), ('Scott', 17)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590aa7a-9959-45d2-9d6c-6e20933e8114",
   "metadata": {},
   "source": [
    "#### How it works...\n",
    "Let's break down the two methods in the preceding code snippet:\r",
    "`\n",
    "sc.parallelize(`) and` take(`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ed790-f35d-4ca1-b817-9f0a19f0382e",
   "metadata": {},
   "source": [
    "##### Spark context parallelize method\n",
    "\n",
    "The sc.parallelize() method is the SparkContext's parallelize method to create a parallelized collection.\n",
    "This allows Spark to distribute the data across multiple nodes, instead of depending on a single node to process the data.\n",
    "\n",
    "Now that we have created myRDD as a parallelized collection, Spark can \n",
    "operate against this data in parallel. Once created, the distributed datase \r\n",
    "(distData) can be operated on in parall.\n",
    "\n",
    "##### .take(...) method\n",
    "\n",
    "Now that you have created your RDD (myRDD), we will use the take() method to return the values to the console (or notebook cell). We will now execute an RDD action, take(). Note that a common approach in PySpark is to use collect(), which returns all values in your RDD from the Spark worker nodes to the driver. There are performance implications when working with a large amount of data as this translates to large volumes of data being transferred from the Spark worker nodes to the driver. For small amounts of data (such as this recipe), this is perfectly fine, but, as a matter of habit, you should pretty much\n",
    "always use the take(n) method instead; it returns the first n elements of the RDD instead of the whole dataset. It is a more efficient method because it first scans one partition and uses those statistics to determine the number of partitions required to return the results.le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83db4db-1b37-4bb1-a6bb-afdb78cb877c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reading data from files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d5957-7fcf-42d3-b16e-62a8422d9e21",
   "metadata": {},
   "source": [
    "We will create an RDD by reading a local file in PySpark.\n",
    "\n",
    "Note that while this recipe is specific \n",
    "to reading local files, a similar syntax can be applied for Hadoop, AWS S3 \r\n",
    "Azure WASBs, and/or Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4577430-8cc6-4cc6-b351-0eb0066563c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/elvin/Documents/Engenharia-De-Dados/Estudos/PySpark/\"\n",
    "\n",
    "myRDD = (sc.textFile(directory + 'airport-codes-na.txt', minPartitions=4, use_unicode=True)).map(lambda element: element.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e472a49b-7150-442e-8e49-c314810b161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['City', 'State', 'Country', 'IATA'],\n",
       " ['Abbotsford', 'BC', 'Canada', 'YXX'],\n",
       " ['Aberdeen', 'SD', 'USA', 'ABR'],\n",
       " ['Abilene', 'TX', 'USA', 'ABI'],\n",
       " ['Akron', 'OH', 'USA', 'CAK']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ceee3b2-31ef-403e-8ae7-40e93b931745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.count() # numero de linhas no RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2842886-79ad-49b3-b635-52d10d4ab037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions() # numero de particoes que suportam este RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435623-38e5-4ce8-bd27-f64962898159",
   "metadata": {},
   "source": [
    "#### How it works...\n",
    "\n",
    "The first code snippet to read the file and return values via take can be \n",
    "broken down into its two components: sc.textFile() and map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80720435-f834-4184-aff7-936d1ed4e70a",
   "metadata": {},
   "source": [
    "##### .textFile(...) method\n",
    "\n",
    "To read the file, we are using SparkContext's textFile() method.\n",
    "\n",
    "Only the first parameter is required, which indicates the location of the text \n",
    "file as per ~/data/flights/airport-codes-na.txt. There are two optiona \r\n",
    "parameters as wel\n",
    "\n",
    "* minPartitions: Indicates the minimum number of partitions that make up \n",
    "the RDD. The Spark engine can often determine the best number o \r\n",
    "partitions based on the file size, but you may want to change t e\r\n",
    "number of partitions for performance reasons and, hence, the ability to\r\n",
    "specify the minimum numb* er.\r\n",
    "use_unicode: Engage this parameter if you are processing Unicode \n",
    "\n",
    "Note that if you were to execute this statement without the subsequent \n",
    "map() function, the resulting RDD would not reference the tab-delimite\n",
    "\n",
    "##### .map(...) method\n",
    "\n",
    "To make sense of the tab-delimiter with an RDD, we will use the \n",
    ".map(...) function to transform the data from a list of strings to a list of list.\n",
    "\n",
    "The key components of this map transformation are:\r",
    "* \n",
    "lambda: An anonymous function (that is, a function defined without  \r\n",
    "name) composed of a single expressio* n\r\n",
    "split: We're using PySpark's split function (within pyspark.sql.functio s)\r\n",
    "to split a string around a regular expression pattern; in this case, our\r\n",
    "delimiter is a tab (that i)s, \\tsrdata.l:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c33d5-24bc-4b77-b478-8ecdb26b534f",
   "metadata": {},
   "source": [
    "## Partitions and performance\n",
    "\n",
    "A key aspect of partitions for your RDD is that the more partitions you\r\n",
    "have, the higher the parallelism. Potentially, having more partitions will\r\n",
    "improve your query performan.ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022c136d-59e8-4efe-8d5a-4c3a4d5bdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.textFile(directory + \"departuredelays.csv\").map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba5ea77-64d1-4f0d-b29d-b92682d96eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1391579"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7acdc6db-ff36-49a7-a398-75b1cf805bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed3c00a3-45a0-4d4a-a641-f5eca66a5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.textFile(directory + \"departuredelays.csv\", minPartitions=8).map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7029530d-e14f-46a3-9d70-213aa273e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1391579"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da4bce-af5e-43bb-a4a1-f80d9537adad",
   "metadata": {},
   "source": [
    "## Overview of RDD transformations\n",
    "\n",
    "There are two types of operation that can be used to shape data in an RDD: transformations and actions. A transformation, as the name suggests, transforms one RDD into another. In other words, it takes an existing RDD and transforms it into one or more output RDDs. . In the preceding recipes, we had used a map() function, which \n",
    "is an example of a transformation to split the data by its tab-delimiter\n",
    "\n",
    "Transformations are lazy (unlike actions). They only get executed when an \n",
    "action is called on an RDD. For example, calling the count()ffunction is a \r\n",
    "acti.on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a2c5eea-cef5-4a9f-905b-8fcaa8a1bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = (sc.textFile(directory + 'airport-codes-na.txt')).map(lambda x: x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2f1ea63-dd79-47db-9378-2b6bc16e557a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['City', 'State', 'Country', 'IATA'],\n",
       " ['Abbotsford', 'BC', 'Canada', 'YXX'],\n",
       " ['Aberdeen', 'SD', 'USA', 'ABR'],\n",
       " ['Abilene', 'TX', 'USA', 'ABI'],\n",
       " ['Akron', 'OH', 'USA', 'CAK']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "695204be-cb9f-4f9d-9262-e55d65da916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = (sc.textFile(directory + 'departuredelays.csv').map(lambda x: x.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cf48604-991f-4803-a9fd-678c9328bec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['date', 'delay', 'distance', 'origin', 'destination'],\n",
       " ['01011245', '6', '602', 'ABE', 'ATL'],\n",
       " ['01020600', '-8', '369', 'ABE', 'DTW'],\n",
       " ['01021245', '-2', '602', 'ABE', 'ATL'],\n",
       " ['01020605', '-4', '602', 'ABE', 'ATL']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829eb96-5f3f-4476-ae57-bd9f4e6369f5",
   "metadata": {},
   "source": [
    "The transformations include the following common tasks:\n",
    "* Removing the header line from your text file: zipWithIndex()\n",
    "* Selecting columns from your RDD: map()\n",
    "* Running a WHERE (filter) clause: filter()\n",
    "* Getting the distinct values: distinct()\n",
    "* Getting the number of partitions: getNumPartitions()\n",
    "* Determining the size of your partitions (that is, the number of elements within each partition): mapPartitionsWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac3763-d871-4d7d-8d8a-d0ecc310b9b3",
   "metadata": {},
   "source": [
    "### .map(...) transformation\n",
    "\n",
    "The map(f) transformation returns a new RDD formed by passing each\r\n",
    "element through a function, f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26fa61f6-9d9f-4bba-a52a-e2c6889ab3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('City', 'State'),\n",
       " ('Abbotsford', 'BC'),\n",
       " ('Aberdeen', 'SD'),\n",
       " ('Abilene', 'TX'),\n",
       " ('Akron', 'OH')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.map(lambda c: (c[0], c[1])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd3e24-561a-4a63-979b-d70c0f56ccd0",
   "metadata": {},
   "source": [
    "### .filter(...) transformation\n",
    "\n",
    "The filter(f) transformation returns a new RDD based on selecting \n",
    "elements for which the f function returns tru.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9574ab2-c309-4e17-b7ae-f486aa1f5ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bellingham', 'WA'),\n",
       " ('Moses Lake', 'WA'),\n",
       " ('Pasco', 'WA'),\n",
       " ('Pullman', 'WA'),\n",
       " ('Seattle', 'WA')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User filter() to filter where second column == \"WA\"\n",
    "airports.map(lambda c: (c[0], c[1])).filter(lambda c: c[1] == \"WA\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019d069-ee34-4e8f-8fc2-413a31158efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
